{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a43144",
   "metadata": {},
   "source": [
    "# Structure answers with OpenAI functions\n",
    "\n",
    "OpenAI functions allows for structuring of response output. This is often useful in question answering when you want to not only get the final answer but also supporting evidence, citations, etc.\n",
    "\n",
    "In this notebook we show how to use an LLM chain which uses OpenAI functions as part of an overall retrieval pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df84ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f059012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd2b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f690f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10b831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"/Users/bytedance/Documents/GitHub/langchain/docs/docs/how_to/state_of_the_union.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "for i, text in enumerate(texts):\n",
    "    text.metadata[\"source\"] = f\"{i}-pl\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f3a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_qa_with_sources_chain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b3e1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a9ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = create_qa_with_sources_chain(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efcdb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_prompt = PromptTemplate(\n",
    "    template=\"Content: {page_content}\\nSource: {source}\",\n",
    "    input_variables=[\"page_content\", \"source\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a08263",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_qa_chain = StuffDocumentsChain(\n",
    "    llm_chain=qa_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=doc_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb876c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA(\n",
    "    retriever=docsearch.as_retriever(), combine_documents_chain=final_qa_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a75bad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"总统是如何谈论俄罗斯的？\"\n",
    "# query =\"What did the president say about russia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a60f109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '总统是如何谈论俄罗斯的？',\n",
       " 'result': '{\"answer\":\"总统在讲话中谴责俄罗斯总统普京的侵略行为，并表示美国将与其他国家合作对俄罗斯实施制裁，释放石油储备以缓解燃油价格上涨，并关闭美国领空对俄罗斯飞机。此外，总统强调美国将提供军事援助、经济援助和人道主义援助给乌克兰，并承诺不会与俄罗斯军队在乌克兰交战，而是为了保卫北约盟友。\",\"sources\":[\"0-pl\",\"2-pl\",\"6-pl\",\"4-pl\"]}'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f93a4",
   "metadata": {},
   "source": [
    "## Using Pydantic\n",
    "\n",
    "If we want to, we can set the chain to return in Pydantic. Note that if downstream chains consume the output of this chain - including memory - they will generally expect it to be in string format, so you should only use this chain when it is the final chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3559727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_pydantic = create_qa_with_sources_chain(llm, output_parser=\"pydantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a7997d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_qa_chain_pydantic = StuffDocumentsChain(\n",
    "    llm_chain=qa_chain_pydantic,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=doc_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79368e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa_pydantic = RetrievalQA(\n",
    "    retriever=docsearch.as_retriever(), combine_documents_chain=final_qa_chain_pydantic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b8641de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '总统是如何谈论俄罗斯的？',\n",
       " 'result': AnswerWithSources(answer='总统在讲话中谈到俄罗斯的侵略行为，强调俄罗斯总统普京的错误判断和对乌克兰的攻击。他提到俄罗斯对乌克兰的侵略是经过预谋的、毫无挑衅的，并指出西方和北约做好了准备，与其他自由国家建立了联盟，共同对抗普京。总统还宣布了对俄罗斯实施制裁的强有力行动，包括释放全球储备的石油以减轻国内的燃油价格，并关闭美国领空对俄罗斯航班。此外，总统强调美国及其盟国将继续支持乌克兰，提供军事援助、经济援助和人道主义援助。', sources=['0-pl', '2-pl', '6-pl', '4-pl'])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_qa_pydantic.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c15395",
   "metadata": {},
   "source": [
    "## Using in ConversationalRetrievalChain\n",
    "\n",
    "We can also show what it's like to use this in the ConversationalRetrievalChain. Note that because this chain involves memory, we will NOT use the Pydantic return type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18e5f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\\n",
    "Make sure to avoid using any unclear pronouns.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "condense_question_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=CONDENSE_QUESTION_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "975c3c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ConversationalRetrievalChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use create_history_aware_retriever together with create_retrieval_chain (see example in docstring) instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "qa = ConversationalRetrievalChain(\n",
    "    question_generator=condense_question_chain,\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    memory=memory,\n",
    "    combine_docs_chain=final_qa_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "784aee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "query = \"总统对Ketanji Brown Jackson说了什么。\"\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfd0ccc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '总统对Ketanji Brown Jackson说了什么。',\n",
       " 'chat_history': [HumanMessage(content='总统对Ketanji Brown Jackson说了什么。'),\n",
       "  AIMessage(content='{\"answer\":\"总统对Ketanji Brown Jackson说: One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\",\"sources\":[\"31-pl\"]}')],\n",
       " 'answer': '{\"answer\":\"总统对Ketanji Brown Jackson说: One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\",\"sources\":[\"31-pl\"]}'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c93f805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"他说了什么关于她的前任?\"\n",
    "# query = \"what did he say about her predecessor?\"\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d8612c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '他说了什么关于她的前任?',\n",
       " 'chat_history': [HumanMessage(content='总统对Ketanji Brown Jackson说了什么。'),\n",
       "  AIMessage(content='{\"answer\":\"总统对Ketanji Brown Jackson说: One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\",\"sources\":[\"31-pl\"]}'),\n",
       "  HumanMessage(content='他说了什么关于她的前任?'),\n",
       "  AIMessage(content='{\"answer\":\"总统对Ketanji Brown Jackson的前任说了Justice Breyer, thank you for your service.\",\"sources\":[\"31-pl\"]}')],\n",
       " 'answer': '{\"answer\":\"总统对Ketanji Brown Jackson的前任说了Justice Breyer, thank you for your service.\",\"sources\":[\"31-pl\"]}'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e4626",
   "metadata": {},
   "source": [
    "## Using your own output schema\n",
    "\n",
    "We can change the outputs of our chain by passing in our own schema. The values and descriptions of this schema will inform the function we pass to the OpenAI API, meaning it won't just affect how we parse outputs but will also change the OpenAI output itself. For example we can add a `countries_referenced` parameter to our schema and describe what we want this parameter to mean, and that'll cause the OpenAI output to include a description of a speaker in the response.\n",
    "\n",
    "In addition to the previous example, we can also add a custom prompt to the chain. This will allow you to add additional context to the response, which can be useful for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f34a48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.chains.openai_functions import create_qa_with_structure_chain\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4d01642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomResponseSchema(BaseModel):\n",
    "#     \"\"\"针对所提出的问题给出一个含有来源的回答。\"\"\"\n",
    "\n",
    "#     answer: str = Field(..., description=\"回答提出的问题\")\n",
    "#     countries_referenced: List[str] = Field(\n",
    "#         ..., description=\"所有在来源中提到的国家\"\n",
    "#     )\n",
    "#     sources: List[str] = Field(\n",
    "#         ..., description=\"回答问题所使用的来源列表\"\n",
    "#     )\n",
    "\n",
    "class CustomResponseSchema(BaseModel):\n",
    "    \"\"\"An answer to the question being asked, with sources.\"\"\"\n",
    "    answer: str = Field(..., description=\"Answer to the question that was asked\")\n",
    "    countries_referenced: List[str] = Field(\n",
    "        ..., description=\"All of the countries mentioned in the sources\"\n",
    "    )\n",
    "    sources: List[str] = Field(\n",
    "        ..., description=\"List of sources used to answer the question\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5647c161",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must provide a pydantic class for schema when output_parser is 'pydantic'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt_messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     SystemMessage(\n\u001b[1;32m      3\u001b[0m         content\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ),\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     16\u001b[0m chain_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate(messages\u001b[38;5;241m=\u001b[39mprompt_messages)\n\u001b[0;32m---> 18\u001b[0m qa_chain_pydantic \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_qa_with_structure_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCustomResponseSchema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpydantic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_prompt\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m final_qa_chain_pydantic \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(\n\u001b[1;32m     22\u001b[0m     llm_chain\u001b[38;5;241m=\u001b[39mqa_chain_pydantic,\n\u001b[1;32m     23\u001b[0m     document_variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     document_prompt\u001b[38;5;241m=\u001b[39mdoc_prompt,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m retrieval_qa_pydantic \u001b[38;5;241m=\u001b[39m RetrievalQA(\n\u001b[1;32m     27\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mdocsearch\u001b[38;5;241m.\u001b[39mas_retriever(), combine_documents_chain\u001b[38;5;241m=\u001b[39mfinal_qa_chain_pydantic\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/chains/openai_functions/qa_with_structure.py:49\u001b[0m, in \u001b[0;36mcreate_qa_with_structure_chain\u001b[0;34m(llm, schema, output_parser, prompt, verbose)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_parser \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpydantic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(schema, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(schema, BaseModel)):\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust provide a pydantic class for schema when output_parser is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m         )\n\u001b[1;32m     53\u001b[0m     _output_parser: BaseLLMOutputParser \u001b[38;5;241m=\u001b[39m PydanticOutputFunctionsParser(\n\u001b[1;32m     54\u001b[0m         pydantic_schema\u001b[38;5;241m=\u001b[39mschema\n\u001b[1;32m     55\u001b[0m     )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m output_parser \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Must provide a pydantic class for schema when output_parser is 'pydantic'."
     ]
    }
   ],
   "source": [
    "prompt_messages = [\n",
    "    SystemMessage(\n",
    "        content=(\n",
    "            \"你是一个世界级算法\"\n",
    "            \"能够以特定格式回答问题。\"\n",
    "        )\n",
    "    ),\n",
    "    HumanMessage(content=\"依据具体的上下文，更好地回答问题。\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{context}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"问题: {question}\"),\n",
    "    HumanMessage(\n",
    "        content=\"提示: 请确保以正确的格式回答问题。以加粗字体列出所有在来源中提到的国家。\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "chain_prompt = ChatPromptTemplate(messages=prompt_messages)\n",
    "\n",
    "qa_chain_pydantic = create_qa_with_structure_chain(\n",
    "    llm=llm, schema=CustomResponseSchema, output_parser=\"pydantic\", prompt=chain_prompt\n",
    ")\n",
    "final_qa_chain_pydantic = StuffDocumentsChain(\n",
    "    llm_chain=qa_chain_pydantic,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=doc_prompt,\n",
    ")\n",
    "retrieval_qa_pydantic = RetrievalQA(\n",
    "    retriever=docsearch.as_retriever(), combine_documents_chain=final_qa_chain_pydantic\n",
    ")\n",
    "query = \"他针对俄罗斯说了什么？\"\n",
    "retrieval_qa_pydantic.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
